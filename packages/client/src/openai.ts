import { randomUUID } from 'node:crypto';
import OpenAI from 'openai';
import { toolCallSchema, type ToolCall } from '@shochan_ai/core';
import {
  isResponseFunctionCallEvent,
  isResponseTextDeltaEvent,
} from './openai-streaming';

type InputMessage =
  | { role: 'user' | 'system' | 'developer'; content: string }
  | OpenAI.Responses.ResponseFunctionToolCall
  | { type: 'function_call_output'; call_id: string; output: string };

type Params = {
  systemPrompt: string;
  inputMessages: InputMessage[];
  tools?: OpenAI.Responses.FunctionTool[];
};

type GenerateToolCallResult = {
  toolCall: ToolCall | null;
  fullOutput: OpenAI.Responses.ResponseOutputItem[];
};

/**
 * Streaming callbacks for real-time token processing
 */
type StreamingCallbacks = {
  /** Callback when tool call is detected */
  onToolCall?: (toolCall: ToolCall) => void;
  /** Callback for each text token (real-time) */
  onTextChunk?: (chunk: string, messageId: string) => void;
};

/**
 * Parameters for generateToolCallWithStreaming
 */
type GenerateToolCallWithStreamingParams = {
  systemPrompt: string;
  inputMessages: InputMessage[];
  tools?: OpenAI.Responses.FunctionTool[];
} & StreamingCallbacks;

/**
 * Error thrown when OpenAI API returns an invalid tool call structure.
 */
export class ToolCallValidationError extends Error {
  constructor(
    public readonly toolCall: unknown,
    public readonly validationErrors: string[],
  ) {
    super(`Invalid tool call from OpenAI API: ${validationErrors.join(', ')}`);
    this.name = 'ToolCallValidationError';
  }
}

export class OpenAIClient {
  private client: OpenAI;

  constructor() {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY is not set in environment variables');
    }

    this.client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  /**
   * Generates a tool call using OpenAI Responses API.
   *
   * @param systemPrompt - System instructions for the model
   * @param inputMessages - Array of input messages including user messages, tool calls, and tool outputs
   * @param tools - Array of available tools (function definitions)
   * @returns Tool call result and full response output
   */
  async generateToolCall({
    systemPrompt,
    inputMessages,
    tools = [],
  }: Params): Promise<GenerateToolCallResult> {
    const maxRetries = 3;
    const baseDelay = 1000; // 1 second

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        const response = await this.client.responses.create({
          model: 'gpt-4o',
          instructions: systemPrompt,
          input: inputMessages as OpenAI.Responses.ResponseInput,
          tools: tools.length > 0 ? tools : undefined,
        });

        const functionCallItem = response.output.find(
          (item): item is OpenAI.Responses.ResponseFunctionToolCall => item.type === 'function_call'
        );

        if (!functionCallItem) {
          return { toolCall: null, fullOutput: response.output };
        }

        const toolCall = this.parseToolCall(functionCallItem);
        return { toolCall, fullOutput: response.output };
      } catch (error) {
        if (this.isRetryableError(error) && attempt < maxRetries) {
          const delay = baseDelay * 2 ** (attempt - 1); // Exponential backoff
          console.log(
            `ðŸ”„ OpenAI API error (attempt ${attempt}/${maxRetries}), retrying in ${delay}ms...`
          );
          await this.sleep(delay);
          continue;
        }

        console.error('OpenAI API error:', error);
        throw error;
      }
    }

    throw new Error('Max retries exceeded for OpenAI API');
  }

  /**
   * Generate tool call with streaming support.
   * Streams text tokens in real-time via callbacks.
   *
   * This method uses OpenAI Responses API with stream: true to receive
   * text tokens as they are generated by the LLM.
   *
   * @param systemPrompt - System instructions
   * @param inputMessages - Input messages
   * @param tools - Available tools
   * @param onToolCall - Callback when tool call is detected
   * @param onTextChunk - Callback for each text token (real-time)
   * @returns Tool call result and full text
   */
  async generateToolCallWithStreaming({
    systemPrompt,
    inputMessages,
    tools = [],
    onToolCall,
    onTextChunk,
  }: GenerateToolCallWithStreamingParams): Promise<{
    toolCall: ToolCall | null;
    fullText: string;
  }> {
    const stream = await this.client.responses.create({
      model: 'gpt-4o',
      instructions: systemPrompt,
      input: inputMessages,
      tools: tools.length > 0 ? tools : undefined,
      stream: true,
    });

    let toolCall: ToolCall | null = null;
    let fullText = '';
    const messageId = randomUUID();

    for await (const rawEvent of stream) {
      const event = rawEvent as unknown;

      if (isResponseFunctionCallEvent(event)) {
        toolCall = this.parseToolCall({
          type: 'function_call',
          call_id: '',
          name: event.name,
          arguments: event.arguments,
        });
        onToolCall?.(toolCall);
      }
      else if (isResponseTextDeltaEvent(event)) {
        const chunk = event.delta || '';
        fullText += chunk;
        onTextChunk?.(chunk, messageId);
      }
      else if (typeof event === 'object' && event !== null && 'type' in event && (event as { type: string }).type === 'error') {
        throw new Error(`OpenAI streaming error: ${JSON.stringify(event)}`);
      }
    }

    return { toolCall, fullText };
  }

  /**
   * Generate text response with streaming support.
   * Used for explaining tool results to the user.
   * 
   * This method does NOT use tools - it only generates text output.
   * Use this after tool execution to provide a natural language explanation.
   *
   * @param systemPrompt - System instructions
   * @param inputMessages - Input messages including tool results
   * @param onTextChunk - Callback for each text token (real-time)
   * @returns Full generated text
   */
  async generateTextWithStreaming({
    systemPrompt,
    inputMessages,
    onTextChunk,
  }: {
    systemPrompt: string;
    inputMessages: InputMessage[];
    onTextChunk?: (chunk: string, messageId: string) => void;
  }): Promise<string> {
    const stream = await this.client.responses.create({
      model: 'gpt-4o',
      instructions: systemPrompt,
      input: inputMessages as OpenAI.Responses.ResponseInput,
      stream: true, // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹
      // tools: undefined (ãƒ„ãƒ¼ãƒ«ãªã— = ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ã¿)
    });

    let fullText = '';
    const messageId = randomUUID();

    for await (const rawEvent of stream) {
      const event = rawEvent as unknown;

      // Handle text delta events - THIS IS WHERE REAL-TIME STREAMING HAPPENS
      if (typeof event === 'object' && event !== null && 'type' in event) {
        const eventType = (event as { type: string }).type;
        
        if (eventType === 'response.output_text.delta') {
          const delta = (event as { delta?: string }).delta || '';
          fullText += delta;
          onTextChunk?.(delta, messageId);
        }
        else if (eventType === 'error') {
          throw new Error(`OpenAI streaming error: ${JSON.stringify(event)}`);
        }
      }
    }

    return fullText;
  }

  /**
   * Parses a function call from OpenAI API response and validates with zod schema.
   * @throws ToolCallValidationError if the tool call doesn't match expected schema
   */
  private parseToolCall(
    functionCall: OpenAI.Responses.ResponseFunctionToolCall
  ): ToolCall {
    const toolCall = {
      intent: functionCall.name,
      parameters: JSON.parse(functionCall.arguments),
    };

    const result = toolCallSchema.safeParse(toolCall);

    if (!result.success) {
      const errors = result.error.issues.map(
        (issue) => `${issue.path.join('.')}: ${issue.message}`
      );
      throw new ToolCallValidationError(toolCall, errors);
    }

    return result.data;
  }

  private isRetryableError(error: unknown): boolean {
    if (error && typeof error === 'object' && 'status' in error) {
      const status = (error as unknown as { status: number }).status;
      // Retry on 429 (rate limit), 500, 502, 503, 504
      return [429, 500, 502, 503, 504].includes(status);
    }
    return false;
  }

  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}
