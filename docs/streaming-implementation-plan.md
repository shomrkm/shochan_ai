# AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè£…è¨ˆç”»ï¼ˆæ”¹è¨‚ç‰ˆï¼‰

## ç›®æ¬¡

1. [æ¦‚è¦](#æ¦‚è¦)
2. [å®Ÿç¾ã—ãŸã„ã“ã¨](#å®Ÿç¾ã—ãŸã„ã“ã¨)
3. [æŠ€è¡“çš„èƒŒæ™¯ã¨èª¿æŸ»çµæœ](#æŠ€è¡“çš„èƒŒæ™¯ã¨èª¿æŸ»çµæœ)
4. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ)
5. [å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—](#å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—)
6. [ãƒ†ã‚¹ãƒˆè¨ˆç”»](#ãƒ†ã‚¹ãƒˆè¨ˆç”»)
7. [ãƒªã‚¹ã‚¯ã¨å¯¾ç­–](#ãƒªã‚¹ã‚¯ã¨å¯¾ç­–)

---

## æ¦‚è¦

### å®Ÿç¾ã—ãŸã„æ©Ÿèƒ½

**ChatGPTã‚„Claudeã®ã‚ˆã†ã«ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§1æ–‡å­—ãšã¤è¡¨ç¤ºã•ã‚Œã‚‹æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™ã€‚**

ç¾åœ¨ã€Shochan AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ï¼ˆã‚¿ã‚¹ã‚¯ä½œæˆã€å–å¾—ãªã©ï¼‰ã‚’å®Ÿè¡Œã—ãŸå¾Œã€æœ€çµ‚çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸€åº¦ã«è¡¨ç¤ºã—ã¾ã™ã€‚ã“ã®æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã€LLMãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã¨åŒæ™‚ã«ãƒ–ãƒ©ã‚¦ã‚¶ã«è¡¨ç¤ºã™ã‚‹ã“ã¨ã§ã€å¿œç­”æ€§ã®é«˜ã„UXã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### ãªãœå¿…è¦ã‹

| ç¾åœ¨ã®å‹•ä½œ | æœ›ã¾ã—ã„å‹•ä½œ |
|-----------|------------|
| ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¾…æ©Ÿ â†’ LLMç”Ÿæˆå®Œäº† â†’ ä¸€åº¦ã«å…¨æ–‡è¡¨ç¤º | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå¾…æ©Ÿ â†’ LLMç”Ÿæˆé–‹å§‹ â†’ **å³åº§ã«**1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤è¡¨ç¤º |
| **Time to First Token (TTFT)**: é…ã„ | **TTFT**: æ•°ç™¾ãƒŸãƒªç§’ |
| ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“: ã€Œå›ºã¾ã£ã¦ã„ã‚‹?ã€ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“: ã€Œå‡¦ç†ä¸­ã ã¨ã‚ã‹ã‚‹ã€ |

---

## å®Ÿç¾ã—ãŸã„ã“ã¨

### ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦–ç‚¹ã§ã®å‹•ä½œ

```
[ãƒ¦ãƒ¼ã‚¶ãƒ¼] ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¦
    â†“
[ã‚·ã‚¹ãƒ†ãƒ ] ğŸ”§ Tool call: create_task
[ã‚·ã‚¹ãƒ†ãƒ ] âœ… Tool executed: create_task
    â†“
[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ] ã‚¿... (å³åº§ã«è¡¨ç¤ºé–‹å§‹)
[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ] ã‚¿ã‚¹ã‚¯...
[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ] ã‚¿ã‚¹ã‚¯ã‚’ä½œ...
[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ] ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¾...
[ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ] ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ (å®Œæˆ)
```

**ãƒã‚¤ãƒ³ãƒˆ**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ**ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§**å°‘ã—ãšã¤è¡¨ç¤ºã•ã‚Œã‚‹

### æŠ€è¡“çš„ç›®æ¨™

1. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**
   - LLMã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã¨åŒæ™‚ã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¸é€ä¿¡
   - Server-Sent Events (SSE) ã‚’ä½¿ç”¨

2. **Time to First Token (TTFT) ã®æœ€å°åŒ–**
   - æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ•°ç™¾ãƒŸãƒªç§’ä»¥å†…ã«è¡¨ç¤ºé–‹å§‹
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å³åº§ã«ã€Œå‡¦ç†ä¸­ã€ã‚’èªè­˜

3. **ä¸»è¦AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®ä¸€è‡´**
   - ChatGPT/Claudeã¨åŒã˜å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¡ç”¨
   - Multi-turn conversationæ–¹å¼

---

## æŠ€è¡“çš„èƒŒæ™¯ã¨èª¿æŸ»çµæœ

### Shochan AI ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web UI    â”‚ (React + Next.js, port 3002)
â”‚ (Frontend)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP POST /api/agent/query
       â”‚ SSE GET /api/stream/:id
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Express API â”‚ (Node.js + TypeScript, port 3001)
â”‚  (Backend)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ Redis (ä¼šè©±çŠ¶æ…‹ã®æ°¸ç¶šåŒ–)
       â”‚
       â””â”€â†’ OpenAI Responses API (LLMå‘¼ã³å‡ºã—)
```

### ä½¿ç”¨æŠ€è¡“

- **OpenAI Responses API**: Function Calling + ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ
- **Server-Sent Events (SSE)**: ã‚µãƒ¼ãƒãƒ¼ã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¸ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šä¿¡
- **better-sse**: Expressç”¨ã®SSEãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- **Redis**: ä¼šè©±çŠ¶æ…‹ã®æ°¸ç¶šåŒ–

### èª¿æŸ»çµæœï¼šOpenAI Responses APIã®ä»•æ§˜

#### é‡è¦ãªç™ºè¦‹

**OpenAI Responses APIã§ã¯ã€Function Callå¾Œã«è‡ªå‹•çš„ã«ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›ã¯ç”Ÿæˆã•ã‚Œãªã„**

å®Ÿéš›ã®APIå‹•ä½œï¼š
```
Turn 1: Function Call
- LLM: create_task({ title: "..." }) ã‚’å‘¼ã³å‡ºã—
- response.function_call_arguments.delta: JSONæ–­ç‰‡
  - "{"
  - "title"
  - ":"
  - "Buy"
  - ...
- response.function_call_arguments.done: å®Œå…¨ãªJSON

Turn 2: Text Output (åˆ¥ã®APIå‘¼ã³å‡ºã—ãŒå¿…è¦)
- LLM: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- response.output_text.delta: ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
  - "ã‚¿"
  - "ã‚¹"
  - "ã‚¯"
  - ...
```

#### ä¸»è¦AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | èª¬æ˜ | æ¡ç”¨ä¾‹ |
|---------|------|--------|
| **Multi-turn Conversation** | ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã€çµæœã‚’å«ã‚ã¦å†åº¦LLMã‚’å‘¼ã³å‡ºã—ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚° | ChatGPT, Claude, OpenAI Agents SDK |
| Single-turn with Message Parameter | Function Callã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚ã‚‹ï¼ˆJSONæ–­ç‰‡ã®ãƒ‘ãƒ¼ã‚¹å¿…è¦ï¼‰ | âŒ éæ¨å¥¨ï¼ˆå®Ÿè£…ãŒè¤‡é›‘ã§ä¸å®‰å®šï¼‰ |

#### ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ï¼ˆOpenAIå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚ˆã‚Šï¼‰

1. **ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’åˆ†é›¢**
   - ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ: éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
   - ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

2. **Multi-turnæ–¹å¼ã®æ¡ç”¨**
   - Turn 1: ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«æ¤œå‡ºãƒ»å®Ÿè¡Œ
   - Turn 2: çµæœã‚’å«ã‚ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

3. **`response.output_text.delta`ã®ä½¿ç”¨**
   - `response.function_call_arguments.delta`ã¯JSONæ–­ç‰‡ï¼ˆä½¿ç”¨ä¸å¯ï¼‰
   - `response.output_text.delta`ãŒçœŸã®ãƒ†ã‚­ã‚¹ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### å…¨ä½“ãƒ•ãƒ­ãƒ¼ï¼ˆMulti-turnæ–¹å¼ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ãƒ¦ãƒ¼ã‚¶ãƒ¼  â”‚ ã€Œã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¦ã€
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â”‚ (1) POST /api/agent/query
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Express API    â”‚ conversationId ã‚’å³åº§ã«è¿”å´
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ (2) ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†é–‹å§‹
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  processAgent   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚ (3) SSEæ¥ç¶šç¢ºèª
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Turn 1: ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«æ¤œå‡ºãƒ»å®Ÿè¡Œ      â”‚
â”‚ OpenAI Responses API            â”‚
â”‚ (stream: false)                 â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â”œâ”€â†’ response: function_call (create_task)
      â”‚   â†’ ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ: NotionToolExecutor
      â”‚   â†’ tool_response ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
      â”‚
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Turn 2: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰â”‚
â”‚ OpenAI Responses API            â”‚
â”‚ (stream: true, tools ãªã—)       â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚
      â””â”€â†’ response.output_text.delta (x Nå›)
          â†’ text_chunk ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ )
          ã€Œã‚¿ã€ã€Œã‚¹ã€ã€Œã‚¯ã€ã€Œã‚’ã€ã€Œä½œæˆã€ã€Œã—ã¾ã€ã€Œã—ãŸã€ã€Œã€‚ã€
            â†“ SSE
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Web UI    â”‚ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºæ›´æ–°
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ã‚¤ãƒ™ãƒ³ãƒˆãƒ•ãƒ­ãƒ¼è©³ç´°

#### Turn 1: ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«æ¤œå‡ºãƒ»å®Ÿè¡Œ

```typescript
// 1. ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ç”Ÿæˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
const response = await openai.responses.create({
  model: 'gpt-4o',
  instructions: systemPrompt,
  input: inputMessages,
  tools: [create_task, get_tasks, ...],
  stream: false, // éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
});

// 2. Function Callã‚’æ¤œå‡º
const functionCall = response.output.find(item => item.type === 'function_call');

// 3. ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
const result = await executor.execute(functionCall);

// 4. SSEã§ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
streamManager.send(conversationId, {
  type: 'tool_call',
  data: functionCall,
});

streamManager.send(conversationId, {
  type: 'tool_response',
  data: result,
});
```

#### Turn 2: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰

```typescript
// 1. ãƒ„ãƒ¼ãƒ«çµæœã‚’å«ã‚ã¦ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
const stream = await openai.responses.create({
  model: 'gpt-4o',
  instructions: 'Explain what you did based on the tool results',
  input: [
    ...previousMessages,
    { type: 'function_call_output', call_id: '...', output: toolResult }
  ],
  stream: true, // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹
  // tools: undefined (ãƒ„ãƒ¼ãƒ«ãªã— = ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ã¿)
});

// 2. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’å—ä¿¡
for await (const event of stream) {
  if (event.type === 'response.output_text.delta') {
    // 3. SSEã§ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é€ä¿¡
    streamManager.send(conversationId, {
      type: 'text_chunk',
      timestamp: Date.now(),
      data: {
        content: event.delta, // â† çœŸã®ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
        messageId: randomUUID(),
      },
    });
  }
}
```

### ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ãƒãƒƒãƒ”ãƒ³ã‚°

| OpenAI Responses API | Shochan AI Event | ãƒ‡ãƒ¼ã‚¿æ§‹é€  | ç”¨é€” |
|---------------------|------------------|----------|------|
| `response.output_text.delta` | `text_chunk` | `{ type: 'text_chunk', data: { content: string, messageId: string } }` | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º |
| `response.completed` | `complete` | `{ type: 'complete', data: { message: string } }` | ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Œäº†é€šçŸ¥ |
| `error` | `error` | `{ type: 'error', data: { error: string, code: string } }` | ã‚¨ãƒ©ãƒ¼é€šçŸ¥ |

---

## å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—

### Phase 0: äº‹å‰æº–å‚™

**ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ç¢ºèª**

```bash
git status
# ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¯ãƒªãƒ¼ãƒ³ãªçŠ¶æ…‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
```

### Phase 1: Core å‹å®šç¾©ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/core/src/types/event.ts`

**ç›®çš„**: `text_chunk` ã‚¤ãƒ™ãƒ³ãƒˆå‹ã‚’å®šç¾©

```typescript
/**
 * Text chunk event - streams text tokens in real-time
 * Used for streaming agent responses after tool execution
 */
export interface TextChunkEvent extends BaseEvent<'text_chunk'> {
  data: {
    /** ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³åˆ†ã¾ãŸã¯è¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ */
    content: string;
    /** ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDï¼ˆåŒä¸€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒãƒ£ãƒ³ã‚¯ã‚’è­˜åˆ¥ï¼‰ */
    messageId: string;
  };
}

// Event union ã«è¿½åŠ 
export type Event =
  | UserInputEvent
  | ToolCallEvent
  | ToolResponseEvent
  | ErrorEvent
  | AwaitingApprovalEvent
  | CompleteEvent
  | TextChunkEvent; // â† è¿½åŠ 

// Type guard è¿½åŠ 
export function isTextChunkEvent(event: Event): event is TextChunkEvent {
  return event.type === 'text_chunk';
}
```

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/core/src/index.ts`

```typescript
export type {
  // ... existing exports
  TextChunkEvent, // â† è¿½åŠ 
} from './types/event';

export {
  // ... existing exports
  isTextChunkEvent, // â† è¿½åŠ 
} from './types/event';
```

**ãƒ“ãƒ«ãƒ‰**:

```bash
pnpm --filter @shochan_ai/core build
```

### Phase 2: OpenAIClient ã®æ‹¡å¼µ

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/client/src/openai.ts`

**ç›®çš„**: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå°‚ç”¨ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ 

```typescript
/**
 * Generate text response with streaming support.
 * Used for explaining tool results to the user.
 * 
 * This method does NOT use tools - it only generates text output.
 * Use this after tool execution to provide a natural language explanation.
 *
 * @param systemPrompt - System instructions
 * @param inputMessages - Input messages including tool results
 * @param onTextChunk - Callback for each text token (real-time)
 * @returns Full generated text
 */
async generateTextWithStreaming({
  systemPrompt,
  inputMessages,
  onTextChunk,
}: {
  systemPrompt: string;
  inputMessages: Array<unknown>;
  onTextChunk?: (chunk: string, messageId: string) => void;
}): Promise<string> {
  const stream = await this.client.responses.create({
    model: 'gpt-4o',
    instructions: systemPrompt,
    input: inputMessages as OpenAI.Responses.ResponseInput,
    stream: true, // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹
    // tools: undefined (ãƒ„ãƒ¼ãƒ«ãªã— = ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ã¿)
  });

  let fullText = '';
  const messageId = randomUUID();

  for await (const rawEvent of stream) {
    const event = rawEvent as unknown;

    // Handle text delta events - THIS IS WHERE REAL-TIME STREAMING HAPPENS
    if (typeof event === 'object' && event !== null && 'type' in event) {
      const eventType = (event as { type: string }).type;
      
      if (eventType === 'response.output_text.delta') {
        const delta = (event as { delta?: string }).delta || '';
        fullText += delta;
        onTextChunk?.(delta, messageId);
      }
      else if (eventType === 'error') {
        throw new Error(`OpenAI streaming error: ${JSON.stringify(event)}`);
      }
    }
  }

  return fullText;
}
```

**ãƒ“ãƒ«ãƒ‰**:

```bash
pnpm --filter @shochan_ai/client build
```

### Phase 3: LLMAgentReducer ã®æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/core/src/agent/llm-agent-reducer.ts`

**ç›®çš„**: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¡ã‚½ãƒƒãƒ‰ã®è¿½åŠ 

#### 3.1 å‹åˆ¶ç´„ã®æ›´æ–°

```typescript
export class LLMAgentReducer<
  TLLMClient extends {
    generateToolCall(params: {
      systemPrompt: string;
      inputMessages: Array<unknown>;
      tools?: Array<unknown>;
    }): Promise<{ toolCall: ToolCall | null }>;

    // â† æ–°è¦è¿½åŠ : ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    generateTextWithStreaming?(params: {
      systemPrompt: string;
      inputMessages: Array<unknown>;
      onTextChunk?: (chunk: string, messageId: string) => void;
    }): Promise<string>;
  },
  TTools extends Array<unknown>,
> implements AgentReducer<Thread, Event>
```

#### 3.2 ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ 

```typescript
/**
 * Generate explanation text with streaming support.
 * Used after tool execution to explain results to the user.
 *
 * @param state - Current thread state
 * @param onTextChunk - Callback for each text token
 * @returns Generated text
 */
async generateExplanationWithStreaming(
  state: Thread,
  onTextChunk?: (chunk: string, messageId: string) => void,
): Promise<string> {
  if (!this.llmClient.generateTextWithStreaming) {
    throw new Error('LLM client does not support text streaming');
  }

  const threadContext = state.serializeForLLM();
  const systemPrompt = `Based on the conversation history and tool results below, provide a natural language explanation to the user.

${threadContext}

Explain what you did and provide a helpful response. Be conversational and respond in the same language the user used.`;

  return await this.llmClient.generateTextWithStreaming({
    systemPrompt,
    inputMessages: [{ role: 'user', content: systemPrompt }],
    onTextChunk,
  });
}
```

**ãƒ“ãƒ«ãƒ‰**:

```bash
pnpm --filter @shochan_ai/core build
```

### Phase 4: Express API ã®æ›´æ–°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/web/src/routes/agent.ts`

**ç›®çš„**: Multi-turnæ–¹å¼ã®å®Ÿè£…

```typescript
async function processAgent(
  conversationId: string,
  deps: AgentDependencies,
): Promise<void> {
  const { redisStore, streamManager, reducer, executor } = deps;
  let iterations = 0;

  try {
    let currentThread = await redisStore.get(conversationId);
    if (!currentThread) {
      throw new Error('Conversation not found');
    }

    console.log(`ğŸ¤– Starting agent processing for: ${conversationId}`);

    // Send connected event
    streamManager.send(conversationId, {
      type: 'connected',
      timestamp: Date.now(),
      data: { status: 'ready', conversationId },
    });

    // Wait for SSE connection
    await new Promise((resolve) => setTimeout(resolve, 500));

    while (true) {
      if (iterations >= MAX_ITERATIONS) {
        throw new Error(`Maximum iterations (${MAX_ITERATIONS}) reached`);
      }
      iterations++;

      // ========================================
      // Turn 1: ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«æ¤œå‡ºãƒ»å®Ÿè¡Œ
      // ========================================
      const toolCallEvent = await reducer.generateNextToolCall(currentThread);

      if (!toolCallEvent) {
        console.error(`âŒ No tool call generated for ${conversationId}`);
        break;
      }

      console.log(`ğŸ”§ Tool call generated: ${toolCallEvent.data.intent}`);
      streamManager.send(conversationId, toolCallEvent);
      currentThread = reducer.reduce(currentThread, toolCallEvent);
      await redisStore.set(conversationId, currentThread);

      const toolCall = toolCallEvent.data;

      // delete_task ã®æ‰¿èªå¾…ã¡
      if (toolCall.intent === 'delete_task') {
        console.log(`âš ï¸  Approval required for: ${conversationId}`);
        const awaitingApprovalEvent: Event = {
          type: 'awaiting_approval',
          timestamp: Date.now(),
          data: toolCall,
        };
        streamManager.send(conversationId, awaitingApprovalEvent);
        currentThread = reducer.reduce(currentThread, awaitingApprovalEvent);
        await redisStore.set(conversationId, currentThread);
        break;
      }

      // ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ
      console.log(`âš™ï¸  Executing tool: ${toolCall.intent}`);
      const result = await executor.execute(toolCall);

      streamManager.send(conversationId, result.event);
      currentThread = reducer.reduce(currentThread, result.event);
      await redisStore.set(conversationId, currentThread);

      console.log(`âœ… Tool executed successfully: ${toolCall.intent}`);

      // ========================================
      // Turn 2: ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
      // ========================================
      console.log(`ğŸ“ Generating explanation with streaming...`);

      await reducer.generateExplanationWithStreaming(
        currentThread,
        (chunk, messageId) => {
          const textChunkEvent: Event = {
            type: 'text_chunk',
            timestamp: Date.now(),
            data: {
              content: chunk,
              messageId,
            },
          };
          streamManager.send(conversationId, textChunkEvent);
        },
      );

      console.log(`âœ… Explanation generated for ${conversationId}`);
      break; // 1ã‚µã‚¤ã‚¯ãƒ«ã§å®Œäº†
    }
  } catch (error) {
    console.error(`âŒ processAgent error for ${conversationId}:`, error);
    streamManager.send(conversationId, {
      type: 'error',
      timestamp: Date.now(),
      data: {
        error: error instanceof Error ? error.message : String(error),
        code: 'AGENT_PROCESSING_FAILED',
      },
    });
  }
}
```

**ãƒ“ãƒ«ãƒ‰**:

```bash
pnpm --filter @shochan_ai/web build
```

### Phase 5: Web UI ã®æ›´æ–°

#### 5.1 å‹å®šç¾©ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/web-ui/types/chat.ts`

```typescript
import type {
  Event,
  ToolCallEvent,
  ToolResponseEvent,
  ErrorEvent,
  CompleteEvent,
  TextChunkEvent, // â† è¿½åŠ 
} from '@shochan_ai/core'

export type {
  Event,
  ToolCallEvent,
  ToolResponseEvent,
  ErrorEvent,
  CompleteEvent,
  TextChunkEvent, // â† è¿½åŠ 
}
```

#### 5.2 SSE ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã®è¿½åŠ 

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/web-ui/lib/sse-client.ts`

```typescript
const SSE_EVENT_TYPES: ReadonlyArray<Event['type'] | 'connected'> = [
  'user_input',
  'tool_call',
  'tool_response',
  'error',
  'awaiting_approval',
  'complete',
  'text_chunk', // â† è¿½åŠ 
  'connected',
] as const
```

#### 5.3 text_chunk ã‚¤ãƒ™ãƒ³ãƒˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**ãƒ•ã‚¡ã‚¤ãƒ«**: `packages/web-ui/components/chat/chat-interface.tsx`

```typescript
const handleSSEEvent = useCallback((event: Event) => {
  let message: Message | null = null

  switch (event.type) {
    case 'text_chunk':
      // â˜… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†
      setMessages((prev) => {
        const lastMessage = prev[prev.length - 1]
        const { messageId, content } = event.data

        // æ—¢å­˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½è¨˜
        if (lastMessage && lastMessage.id === messageId) {
          return [
            ...prev.slice(0, -1),
            { ...lastMessage, content: lastMessage.content + content },
          ]
        }

        // æ–°è¦ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
        return [
          ...prev,
          {
            id: messageId,
            type: 'agent' as const,
            content,
            timestamp: event.timestamp,
          },
        ]
      })
      return

    case 'tool_call':
      message = createToolCallMessage(event)
      break

    case 'tool_response':
      message = createToolResponseMessage(event)
      break

    case 'complete':
      message = createCompleteMessage(event)
      break

    case 'error':
      message = createErrorMessage(event)
      break
  }

  if (message) {
    setMessages((prev) => [...prev, message])
  }
}, [])
```

**ãƒ“ãƒ«ãƒ‰**:

```bash
pnpm --filter @shochan_ai/web-ui build
```

### Phase 6: çµ±åˆãƒ“ãƒ«ãƒ‰ã¨ãƒ†ã‚¹ãƒˆ

```bash
# å…¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ“ãƒ«ãƒ‰
pnpm -r build

# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
pnpm --filter @shochan_ai/web dev  # port 3001

# UIèµ·å‹•ï¼ˆåˆ¥ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼‰
pnpm --filter @shochan_ai/web-ui dev  # port 3002
```

---

## ãƒ†ã‚¹ãƒˆè¨ˆç”»

### ãƒ†ã‚¹ãƒˆç’°å¢ƒ

- Redis: `redis://localhost:6379`
- Express API: `http://localhost:3001`
- Web UI: `http://localhost:3002`

### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹

#### 1. åŸºæœ¬çš„ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‹•ä½œ

**æ‰‹é †**:
1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:3002` ã‚’é–‹ã
2. ã€Œä»Šæ—¥ã®ã‚¿ã‚¹ã‚¯ã‚’æ•™ãˆã¦ã€ã¨å…¥åŠ›
3. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡

**æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ**:
- [ ] `ğŸ”§ Tool call: get_tasks` ãŒå³åº§ã«è¡¨ç¤º
- [ ] ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒ**å°‘ã—ãšã¤**è¡¨ç¤ºã•ã‚Œã‚‹
- [ ] ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå®Œæˆã™ã‚‹ã¾ã§æ•°ç§’ã‹ã‹ã‚‹
- [ ] æœ€çµ‚çš„ã«å®Œå…¨ãªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹

**ç¢ºèªãƒã‚¤ãƒ³ãƒˆ**:
- Time to First Token (TTFT) ãŒ1ç§’ä»¥å†…
- ãƒ†ã‚­ã‚¹ãƒˆãŒæ»‘ã‚‰ã‹ã«è¿½åŠ ã•ã‚Œã‚‹
- ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„

#### 2. ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«é€£é–

**æ‰‹é †**:
1. ã€Œæ˜æ—¥ã®äºˆå®šã§ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¦ã€ã¨å…¥åŠ›

**æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ**:
- [ ] `ğŸ”§ Tool call: create_task`
- [ ] `âœ… Tool executed`
- [ ] ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º

#### 3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

**æ‰‹é †**:
1. OpenAI APIã‚­ãƒ¼ã‚’ç„¡åŠ¹åŒ–
2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡

**æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ**:
- [ ] `âŒ Error: ...` ãŒè¡¨ç¤ºã•ã‚Œã‚‹
- [ ] ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„

#### 4. SSEæ¥ç¶šã®ç¢ºèª

**é–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ç¢ºèª**:
```
Network ã‚¿ãƒ– â†’ stream â†’ Event Stream
```

**æœŸå¾…ã•ã‚Œã‚‹ã‚¤ãƒ™ãƒ³ãƒˆ**:
```
event: connected
data: {"type":"connected","timestamp":...}

event: tool_call
data: {"type":"tool_call", ...}

event: tool_response
data: {"type":"tool_response", ...}

event: text_chunk
data: {"type":"text_chunk","data":{"content":"ã‚¿",...}}

event: text_chunk
data: {"type":"text_chunk","data":{"content":"ã‚¹",...}}
```

---

## ãƒªã‚¹ã‚¯ã¨å¯¾ç­–

### ãƒªã‚¹ã‚¯1: APIå‘¼ã³å‡ºã—å›æ•°ã®å¢—åŠ 

**å½±éŸ¿**: Multi-turnæ–¹å¼ã«ã‚ˆã‚Šã€1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Š2å›ã®APIå‘¼ã³å‡ºã—ãŒå¿…è¦

**é‡è¦åº¦**: ğŸŸ¡ ä¸­

**å¯¾ç­–**:
- ã‚³ã‚¹ãƒˆç›£è¦–ã®å®Ÿè£…
- å¿…è¦ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®æ¤œè¨
- ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æœ€é©åŒ–

**ã‚³ã‚¹ãƒˆè©¦ç®—**:
```
å¾“æ¥: 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆ = 1 APIå‘¼ã³å‡ºã—
æ–°æ–¹å¼: 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆ = 2 APIå‘¼ã³å‡ºã—ï¼ˆãƒ„ãƒ¼ãƒ«æ¤œå‡º + ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼‰
å¢—åŠ ç‡: ç´„2å€

ãŸã ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã«ã‚ˆã‚‹UXå‘ä¸Šã®ãƒ¡ãƒªãƒƒãƒˆã¯å¤§ãã„
```

### ãƒªã‚¹ã‚¯2: SSEæ¥ç¶šã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°å•é¡Œ

**å½±éŸ¿**: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹å‰ã«SSEæ¥ç¶šãŒç¢ºç«‹ã•ã‚Œã¦ã„ãªã„ â†’ åˆæœŸãƒãƒ£ãƒ³ã‚¯ã®æå¤±

**é‡è¦åº¦**: ğŸŸ¡ ä¸­

**å¯¾ç­–**:
- `connected` ã‚¤ãƒ™ãƒ³ãƒˆã«ã‚ˆã‚‹æ¥ç¶šç¢ºèª
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ã®å®Ÿè£…ï¼ˆ500mså¾…æ©Ÿï¼‰
- æ¥ç¶šçŠ¶æ…‹ã®UIè¡¨ç¤º

### ãƒªã‚¹ã‚¯3: ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯

**å½±éŸ¿**: é•·æ™‚é–“ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ 

**é‡è¦åº¦**: ğŸŸ¢ ä½

**å¯¾ç­–**:
- ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®é©åˆ‡ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- `for await` ãƒ«ãƒ¼ãƒ—ã®çµ‚äº†ç¢ºèª
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

### ãƒªã‚¹ã‚¯4: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ‡æ–­æ™‚ã®å†æ¥ç¶š

**å½±éŸ¿**: SSEæ¥ç¶šãŒåˆ‡æ–­ã•ã‚ŒãŸå ´åˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ°—ã¥ã‹ãªã„

**é‡è¦åº¦**: ğŸŸ¢ ä½

**å¯¾ç­–**:
- EventSource ã®è‡ªå‹•å†æ¥ç¶šæ©Ÿèƒ½ã‚’æ´»ç”¨
- æ¥ç¶šçŠ¶æ…‹ã®UIè¡¨ç¤º
- å†æ¥ç¶šæ™‚ã®çŠ¶æ…‹å¾©å…ƒ

---

## å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### å¿…é ˆé …ç›®

- [ ] **Phase 0**: GitçŠ¶æ…‹ã®ç¢ºèª
- [ ] **Phase 1**: Coreå‹å®šç¾©ã®è¿½åŠ ï¼ˆTextChunkEventï¼‰
- [ ] **Phase 2**: OpenAIClientã®æ‹¡å¼µï¼ˆgenerateTextWithStreamingï¼‰
- [ ] **Phase 3**: LLMAgentReducerã®æ›´æ–°ï¼ˆgenerateExplanationWithStreamingï¼‰
- [ ] **Phase 4**: Express APIã®æ›´æ–°ï¼ˆMulti-turnå®Ÿè£…ï¼‰
- [ ] **Phase 5**: Web UIã®æ›´æ–°ï¼ˆtext_chunkãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰
- [ ] **Phase 6**: çµ±åˆãƒ“ãƒ«ãƒ‰ã¨ãƒ†ã‚¹ãƒˆ

### ãƒ†ã‚¹ãƒˆé …ç›®

- [ ] åŸºæœ¬çš„ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‹•ä½œï¼ˆTTFT < 1ç§’ï¼‰
- [ ] ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«é€£é–
- [ ] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- [ ] SSEæ¥ç¶šã®ç¢ºèªï¼ˆé–‹ç™ºè€…ãƒ„ãƒ¼ãƒ«ï¼‰
- [ ] é•·æ™‚é–“ç¨¼åƒã§ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

---

## å‚è€ƒè³‡æ–™

- [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses)
- [OpenAI Streaming Guide](https://platform.openai.com/docs/guides/streaming)
- [OpenAI Agents SDK - Streaming](https://openai.github.io/openai-agents-python/streaming/)
- [Server-Sent Events (MDN)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [better-sse](https://github.com/MatthewWid/better-sse)

---

## ã¾ã¨ã‚

ã“ã®å®Ÿè£…ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã‚’é”æˆã—ã¾ã™ï¼š

âœ… **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**: LLMã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã¨åŒæ™‚ã«ãƒ–ãƒ©ã‚¦ã‚¶ã¸è¡¨ç¤º
âœ… **TTFTæœ€å°åŒ–**: æ•°ç™¾ãƒŸãƒªç§’ã§æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡¨ç¤º
âœ… **ä¸»è¦AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®ä¸€è‡´**: ChatGPT/Claudeã¨åŒã˜Multi-turnæ–¹å¼
âœ… **å®‰å®šæ€§**: æ¨™æº–çš„ãªAPIä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å®Ÿè£…ãŒå®‰å®š
âœ… **å‹å®‰å…¨æ€§**: TypeScriptã®å‹ã‚·ã‚¹ãƒ†ãƒ ã‚’æ´»ç”¨

### å®Ÿè£…æ–¹å¼ã®é¸æŠç†ç”±

**Multi-turn Conversationæ–¹å¼ã‚’æ¡ç”¨**

| é …ç›® | è©•ä¾¡ | èª¬æ˜ |
|------|------|------|
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§** | âœ… å®Œå…¨ | LLMç”Ÿæˆã¨åŒæ™‚ã«è¡¨ç¤º |
| **å®Ÿè£…ã®è¤‡é›‘ã•** | âœ… ã‚·ãƒ³ãƒ—ãƒ« | æ¨™æº–çš„ãªAPIä½¿ç”¨ |
| **å®‰å®šæ€§** | âœ… é«˜ã„ | JSONãƒ‘ãƒ¼ã‚¹ä¸è¦ |
| **ä¸»è¦AIã¨ã®ä¸€è‡´** | âœ… å®Œå…¨ä¸€è‡´ | ChatGPT/Claudeã¨åŒã˜ |
| **ã‚³ã‚¹ãƒˆ** | âš ï¸ 2å€ | APIå‘¼ã³å‡ºã—ãŒ2å› |

**çµè«–**: ã‚³ã‚¹ãƒˆã¯å¢—åŠ ã™ã‚‹ãŒã€UXå‘ä¸Šã¨ã‚³ãƒ¼ãƒ‰ã®å®‰å®šæ€§ã®ãƒ¡ãƒªãƒƒãƒˆãŒå¤§ãã„ãŸã‚ã€Multi-turnæ–¹å¼ã‚’æ¡ç”¨ã—ã¾ã™ã€‚

å®Ÿè£…å¾Œã€Shochan AIã¯ä¸»è¦AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆChatGPT/Claudeï¼‰ã¨åŒç­‰ã®å¿œç­”æ€§ã¨ä¿¡é ¼æ€§ã‚’æŒã¤ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚
